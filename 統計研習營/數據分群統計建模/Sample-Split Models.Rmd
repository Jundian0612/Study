---
title: "Sample-Split Models"
author: "Jundian"
date: "2025-07-22"
output: html_document
---

# Gaussian mixture model (GMM)
```{r}
library(mclust)
set.seed(123)
n = 1000
pi = 0.4
m1 = 0
s1 = 1
m2 = 5
s2 = 1.5
z = rbinom(n, 1, prob = pi)
y = ifelse(z == 1,rnorm(n, mean = m1, sd = s1),rnorm(n, mean = m2, sd = s2))
fit.GMM = Mclust(y, G = 2, modelNames = "V")
sorted_idx = order(fit.GMM$parameters$mean)
pi_hat = fit.GMM$parameters$pro[sorted_idx]
mu_hat = fit.GMM$parameters$mean[sorted_idx]
sd_hat = sqrt(fit.GMM$parameters$variance$sigmasq)[sorted_idx]
```

# Functions of Logistic Regression
```{r}
# Load necessary libraries
library(ggplot2)
# Set seed for reproducibility
set.seed(123)
# Simulate data
n = 1000
covariate1 = runif(n)
covariate2 = runif(n)
intercept = rep(1, n)
# Simulated outcome using logistic regression formula
outcome = rbinom(n, 1, plogis(2 * covariate1 + 3 * covariate2- 2))
# Create dataframe
data = data.frame(covariate1, covariate2, intercept, outcome)
# Fit logistic regression model
model = glm(outcome ~ covariate1 + covariate2, data = data, family = binomial)
intercept =-coef(model)[1] / coef(model)[3]
slope =-coef(model)[2] / coef(model)[3]
# Generate prediction grid
seq.x = seq(0,1,length=100)
pred_grid = expand.grid(covariate1 = seq.x, covariate2 = seq.x)
# Predict probabilities using the model
pred_probs = predict(model, newdata = pred_grid, type = "response")
# Create dataframe for prediction grid
pred_df = cbind(pred_grid, pred_probs)
# Plot classification result with logistic regression decision boundary
ggplot(data = pred_df, aes(x = covariate1, y = covariate2, fill = pred_probs)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "black") +
# Add logistic regression decision boundary
geom_abline(intercept = intercept, slope = slope, color = "black", linetype = "dashed") +
geom_abline(intercept = 2/3, slope =-2/3, color = "grey", linetype = "solid") +
labs(x = "Covariate 1", y = "Covariate 2", title = "Classification Result") +
theme_minimal()
```

# Functions of (Weighted) Support Vector Machines
```{r}
# Load required library
library(WeightSVM)
# Generate example data
set.seed(123)
x1 = c(rnorm(50, mean =-1), rnorm(50, mean = 1))
x2 = c(rnorm(50, mean = 1), rnorm(50, mean =-1))
y = c(rep(-1, 50), rep(1, 50))
data = data.frame(x1 = x1, x2 = x2, y = factor(y))
# Fit standard soft-margin SVM
model = wsvm(x = data[, c("x1", "x2")], y = data$y, weight = rep(1, 100),
kernel="linear",cost=10,scale=FALSE)
cf = coef(model)
# plot data and separating hyperplane
plot(x2~x1,data=data[which(data[,3]==1),],col=y,pch=1,main="Standard SVM")
points(x2~x1,data=data[which(data[,3]==-1),],col=y,pch=2)
abline(-cf[1]/cf[3],-cf[2]/cf[3],col="red")
abline(-(cf[1] + 1)/cf[3],-cf[2]/cf[3], col = "blue",lty="dashed")
abline(-(cf[1]- 1)/cf[3],-cf[2]/cf[3], col = "blue",lty="dashed")
sv_data = data[rownames(model$SV),]
points(x2~x1,data=sv_data[which(sv_data[,3]==-1),],col=y,pch=17)
points(x2~x1,data=sv_data[which(sv_data[,3]==1),],col=y,pch=16)
# Define different weights for the positive and negative samples
weights = ifelse(y == 1, 3, 1)
# Fit weighted soft-margin SVM
modelw = wsvm(x = data[, c("x1", "x2")], y = data$y, weight = weights ,
kernel="linear",cost=10,scale=FALSE)
cf = coef(modelw)
# plot data and separating hyperplane
plot(x2~x1,data=data[which(data[,3]==1),],col=y,pch=1,main="Weighted SVM")
points(x2~x1,data=data[which(data[,3]==-1),],col=y,pch=2)
abline(-cf[1]/cf[3],-cf[2]/cf[3],col="red")
abline(-(cf[1] + 1)/cf[3],-cf[2]/cf[3], col = "blue",lty="dashed")
abline(-(cf[1]- 1)/cf[3],-cf[2]/cf[3], col = "blue",lty="dashed")
sv_data = data[rownames(modelw$SV),]
points(x2~x1,data=sv_data[which(sv_data[,3]==-1),],col=y,pch=17)
points(x2~x1,data=sv_data[which(sv_data[,3]==1),],col=y,pch=16)
```
